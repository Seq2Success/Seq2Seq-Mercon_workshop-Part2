{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPcrhGHgrebXG9gn7YAuvRz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tokenizers\n","!pip install transformers"],"metadata":{"id":"6gVCvCff05tZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizers and Vocabulary\n","\n","Tokenizer does the job of breaking the datapoint to smallest units available for the task. The smallest units are also known as ``Tokens``. The process of breaking into small units is called ``Tokenization``.  \n","\n","``Vocabulary`` contains all the unique tokens created by the Tokenizer.\n","\n","There are 3 broad categories of Tokenizers based on how they split a text input.\n","\n","\n","1.   Word Level\n","2.   Subword Level\n","3.   Character Level\n","\n"],"metadata":{"id":"RnjLj_Hsv-PQ"}},{"cell_type":"markdown","source":["Lets see how we perform this tokenization. For this demo we will use english data set from tatoeba.org.\n","\n","Link : [here](https://tatoeba.org/)\n","\n","## Word-Level Tokenization"],"metadata":{"id":"GSXJyJodyWEl"}},{"cell_type":"code","source":["lines = [\"I trained the model using the train dataset and used the test data set to test it\",\n","        \"I ate a dumpling yesterday\",\n","        \"The duck and the duckling swam across the lake\",\n","        ]"],"metadata":{"id":"2a-qn4vDKMuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lets clean it a bit\n","text = \" \".join(lines)"],"metadata":{"id":"GA3rSsiTywuh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lets now create word level tokenization\n","tokens = text.split()\n","print(tokens[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOpqnYCtzLaQ","executionInfo":{"status":"ok","timestamp":1699534808345,"user_tz":-330,"elapsed":323,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"cf94cc11-698c-4703-b997-5486b8e622b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'trained', 'the', 'model', 'using', 'the', 'train', 'dataset', 'and', 'used']\n"]}]},{"cell_type":"code","source":["vocabulary_wordlevel = set(tokens)\n","print(f\"Length of the vocabulary: {len(vocabulary_wordlevel)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JGDCtNOfzVj0","executionInfo":{"status":"ok","timestamp":1699534874318,"user_tz":-330,"elapsed":494,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"123013f8-54a2-4dbf-a08d-9222e75c682b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of the vocabulary: 24\n"]}]},{"cell_type":"markdown","source":["## Subword Level Tokenization"],"metadata":{"id":"1JlUKCt908uN"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","def pretrained_subword_tokenize(lines, model_name=\"bert-base-uncased\"):\n","    # Load the pretrained tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    unique_tokens = []\n","    for line in lines:\n","      # Tokenize the text into subwords\n","      subword_tokens = tokenizer.tokenize(line)\n","      unique_tokens = list(unique_tokens + subword_tokens)\n","    return unique_tokens\n","\n","def tokenize(line, model_name=\"bert-base-uncased\"):\n","    # Load the pretrained tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    subword_tokens = tokenizer.tokenize(line)\n","    return subword_tokens"],"metadata":{"id":"urPPa3C708Us"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_subword_level = pretrained_subword_tokenize(lines)\n","vocab_subword_level = set([subword.replace(\"##\",\"\") for subword in vocab_subword_level])\n","\n","print(vocab_subword_level)\n","print(len(vocab_subword_level))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Tj_hunT0nN0","executionInfo":{"status":"ok","timestamp":1699535275457,"user_tz":-330,"elapsed":3,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"211e583e-360f-4e31-a16c-9b5960177d1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'used', 'data', 'ate', 'to', 'ling', 'lake', 'model', 'yesterday', 'swam', 'duck', 'the', 'it', 'test', 'and', 'using', 'dump', 'set', 'i', 'across', 'train', 'a', 'trained'}\n","22\n"]}]},{"cell_type":"markdown","source":["## Character Level Tokenization"],"metadata":{"id":"f9e9F6vLLr9W"}},{"cell_type":"code","source":["tokens = list(text.lower())\n","vocab_character_level = set(tokens)\n","\n","print(vocab_character_level)\n","print(len(vocab_character_level))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZ-sg9uNLxUh","executionInfo":{"status":"ok","timestamp":1699535247625,"user_tz":-330,"elapsed":2,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"bd95740c-7ac6-4632-e006-9eb2ee1932a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'c', 'r', 'd', 'u', 'n', 's', 'e', 'p', 'h', 'o', 't', 'g', 'y', 'w', ' ', 'i', 'k', 'a', 'm', 'l'}\n","20\n"]}]},{"cell_type":"markdown","source":["**Observetion**\n","\n","It is clear that Word level tokenization will have the largest size out of the three methods, while subword tokenization has an intermediate size and finally the character level tokenization will always have the lowest.\n","\n","So why does the size of the vocabulary matter so much?\n","\n","This because our RNN or any other language model is a probabilistic model, it assigns a probability for what could be the next token given the current information. This will be clearly understood in our decoding tutorial."],"metadata":{"id":"G3CtBhypNN9D"}},{"cell_type":"code","source":[],"metadata":{"id":"gy5WQQwSL6ad"},"execution_count":null,"outputs":[]}]}