{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPBVZelAyUs+o0VFC0Bb87e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QM9z_laQEoEH","executionInfo":{"status":"ok","timestamp":1699671100317,"user_tz":-330,"elapsed":323,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"0e51bcd9-cf20-48de-ce44-fefab633bb65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Probability Vector: tensor([0.0424, 0.0378, 0.0395, 0.1346, 0.0567, 0.4819, 0.0170, 0.0322, 0.1298,\n","        0.0281])\n","Sum of Probabilities: 0.9999999403953552\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","\n","def model(inputs,context, vocab_size):\n","    # Set random seed to show influence of the previous token\n","    torch.manual_seed(inputs+context)\n","\n","    # Generate random weights for the model\n","    output_hidden_layer = torch.randn(vocab_size)\n","\n","    # Apply softmax to get a probability distribution\n","    probabilities = F.softmax(output_hidden_layer, dim=0)\n","\n","    return probabilities\n","\n","# Example usage:\n","random_seed = 42\n","vocabulary_size = 10\n","probability_vector = model(2,3, vocabulary_size)\n","\n","print(\"Probability Vector:\", probability_vector)\n","print(\"Sum of Probabilities:\", torch.sum(probability_vector).item())\n"]},{"cell_type":"code","source":["#now lets code the greedy decoding path\n","vocab = [\"<SOS>\",\"<EOS>\", \"a\",\"b\",\"c\"]\n","vocab_size = len(vocab)\n","our_input = \"a b c\"\n","decoder_input = vocab.index(\"<SOS>\")\n","context_vector = torch.randint(1, 1000, (1,)).item()\n","print(f\"Context vector: {context_vector}\")\n","\n","print(f\"Tokenized representation: {decoder_input}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pX3M5CkdFj_q","executionInfo":{"status":"ok","timestamp":1699671122532,"user_tz":-330,"elapsed":2,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"e0eae1c5-fa7c-444f-8756-2466ce6b9711"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Context vector: 256\n","Tokenized representation: 0\n"]}]},{"cell_type":"code","source":["model_ouput_1 = model(decoder_input, context_vector,vocab_size)\n","print(\"First Output by Model: \",model_ouput_1)\n","\n","#lets take the token index of the token with the max probility\n","max_probability_index = torch.argmax(model_ouput_1).item()\n","print(f\"Index with max probability: {max_probability_index}\")\n","\n","#lets decode this\n","print(f\"Decoded output: {vocab[max_probability_index]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hGoUl3JOIVvV","executionInfo":{"status":"ok","timestamp":1699671124337,"user_tz":-330,"elapsed":338,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"dcca8baa-bcb0-4cfe-c549-4a982bae7830"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["First Output by Model:  tensor([0.1550, 0.0538, 0.1583, 0.4801, 0.1528])\n","Index with max probability: 3\n","Decoded output: b\n"]}]},{"cell_type":"code","source":["#lets try do it again\n","model_ouput_2 = model(max_probability_index, context_vector,vocab_size)\n","\n","#lets take the token index of the token with the max probility\n","max_probability_index = torch.argmax(model_ouput_2).item()\n","print(f\"Index with max probability: {max_probability_index}\")\n","\n","#lets decode this\n","print(f\"Decoded output: {vocab[max_probability_index]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eR8hUGctOFol","executionInfo":{"status":"ok","timestamp":1699671126738,"user_tz":-330,"elapsed":513,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"101a95d9-ef44-4556-a978-5fa56a849e5a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Index with max probability: 2\n","Decoded output: a\n"]}]},{"cell_type":"code","source":["decoder_input = vocab.index(\"<SOS>\")\n","context_vector = torch.randint(1, 1000, (1,)).item()\n","#now lets loop this\n","while True:\n","  model_ouput = model(decoder_input, context_vector,vocab_size)\n","  decoder_input = torch.argmax(model_ouput).item()\n","  print(f\"Decoded output: {vocab[decoder_input]}\")\n","  if vocab[decoder_input] == \"<EOS>\":\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qrUQsyTRp0K","executionInfo":{"status":"ok","timestamp":1699671382112,"user_tz":-330,"elapsed":328,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"78af09fe-8b5e-433d-8202-489895ef2b10"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoded output: c\n","Decoded output: <EOS>\n"]}]},{"cell_type":"code","source":["MAX_LENGTH = 5\n","decoder_input = vocab.index(\"<SOS>\")\n","context_vector = torch.randint(1, 1000, (1,)).item()\n","#now lets loop this\n","for i in range(MAX_LENGTH):\n","  model_ouput = model(decoder_input, context_vector,vocab_size)\n","  decoder_input = torch.argmax(model_ouput).item()\n","  print(f\"Decoded output: {vocab[decoder_input]}\")\n","  if vocab[decoder_input] == \"<EOS>\":\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Flv37S9TW4k","executionInfo":{"status":"ok","timestamp":1699671466006,"user_tz":-330,"elapsed":315,"user":{"displayName":"machine quest","userId":"08138845656426892373"}},"outputId":"419e8da8-1fa0-49be-bd09-6e13fa34b357"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoded output: c\n","Decoded output: <EOS>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xQBCmRNzT_9I"},"execution_count":null,"outputs":[]}]}